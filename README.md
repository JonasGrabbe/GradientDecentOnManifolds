# Stochastic Gradient Descent on Riemannian Manifolds.

This project is inspired by the works of Silvere Bonnabel on "Stochastic gradient descent on Riemannian manifolds" [1] and S. Chan et al.'s work on "Decentralized Riemannian Gradient Descent on the Stiefel Manifold"[2]. The goal of this project is to implement and explore the applications of Riemannian stochastic gradient descent algorithms in various optimization problems.

### Background
Stochastic gradient descent (SGD) is a widely used optimization technique for finding the local minima of a cost function, especially in the context of machine learning. Bonnabel's work extends the SGD algorithm to cases where the function is defined on a Riemannian manifold [1]. 
This extension has numerous potential applications, including gossip algorithms on the set of covariance matrices and optimization on the Stiefel manifold [2].

### Project Structure
The project is organized into the following directories:




### References
[1] Bonnabel, S. (2013). Stochastic gradient descent on Riemannian manifolds. arXiv preprint arXiv:1111.5280. Link
[2] Chan, S., et al. (2021). Decentralized Riemannian Gradient Descent on the Stiefel Manifold. arXiv preprint arXiv:2102.0709. Link

